import os
import time
import requests
import urllib3
from queue import Queue
from parser import parse_and_save, extract_links
from db_process import get_db_connection

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_DIR = "/home/woogaxon/WORK/DEV/RAG"
os.chdir(BASE_DIR)

TARGET_FILE = os.path.join(BASE_DIR, "DATA", "targets.txt")
SAVE_DIR = os.path.join(BASE_DIR, "DATA", "pages")
os.makedirs(SAVE_DIR, exist_ok=True)

MAX_DEPTH = 3

  # âœ… depth=0ì´ë©´ ì´ˆê¸° URLë§Œ, depth=1ì´ë©´ ë§í¬ í•œ ë²ˆ ë”°ë¼ê°
MAX_DOCS_PER_DEPTH = 100  # ëª¨ë“  depthì— ë™ì¼í•˜ê²Œ ì ìš©

depth_counts = {}  # ê° depthë³„ ë¬¸ì„œ ìˆ˜ ì¶”ì 
url_queue = Queue()
visited = set()




def load_targets():
    with open(TARGET_FILE, "r") as f:
        for line in f:
            url = line.strip()
            # ğŸ” ë¹ˆ ì¤„ ë˜ëŠ” ì£¼ì„(#)ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ì€ ìŠ¤í‚µ
            if not url or url.startswith("#"):
                continue
            url_queue.put((url, 0))  # ì´ˆê¸° depthëŠ” 0

def crawl_and_parse():
    conn = get_db_connection()
    while not url_queue.empty():
        url, depth = url_queue.get()

        if url in visited or depth > MAX_DEPTH:
            continue

        visited.add(url)

        try:
            res = requests.get(url, verify=False, timeout=10)
            if res.status_code == 200:
                print(f"[CRAWLED] {url} (depth={depth})")
                parse_and_save(res.text, url, conn, SAVE_DIR)

                # ë‹¤ìŒ depthë¡œ ìµœëŒ€ 10ê°œ ë§í¬ë§Œ ì¶”ì¶œ
                if depth < MAX_DEPTH:
                    links = extract_links(res.text, url)
                    limited_links = list(links)[:8]  # âœ… ì—¬ê¸°ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    for link in limited_links:
                        if link not in visited:
                            url_queue.put((link, depth + 1))
        except Exception as e:
            print(f"[ERROR] {url}: {e}")
    conn.close()



if __name__ == "__main__":
    load_targets()
    crawl_and_parse()
