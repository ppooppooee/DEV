from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, unquote 
import os

def parse_and_save(html, url, save_dir):
    soup = BeautifulSoup(html, "html.parser")
    title = soup.title.string.strip() if soup.title else "No Title"

    # ğŸ” ë§í¬ ì¶”ì¶œ (hrefë§Œ)
    links = [a.get("href") for a in soup.find_all("a", href=True)]

    # ğŸ”§ ë„¤ë¹„ê²Œì´ì…˜ ìš”ì†Œ ì œê±°
    for tag in soup(["nav", "footer", "aside", "header", "script", "style"]):
        tag.decompose()

    # ğŸ” ë³¸ë¬¸ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„: article > main > content div)
    content = None
    for selector in ["article", "main", "div#content", "div.post", "section"]:
        content = soup.select_one(selector)
        if content:
            break
    if not content:
        content = soup.body  # fallback

    # ğŸ”§ ë§í¬ í…ìŠ¤íŠ¸ ì œê±°
    for a in content.find_all("a"):
        a.unwrap()  # ë§í¬ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸°ì§€ ì•Šê³  ì œê±°

    # í…ìŠ¤íŠ¸ ì •ë¦¬
    raw_text = content.get_text()
    cleaned_lines = [line for line in raw_text.splitlines() if line.strip()]
    cleaned_text = "\n".join(cleaned_lines)

    # íŒŒì¼ëª… ìƒì„±
    decoded_url = unquote(url)
    filename = decoded_url.replace("https://", "").replace("http://", "").replace("/", "_") + ".txt"
    filepath = os.path.join(save_dir, filename)

    # ì €ì¥
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"URL: {url}\n")
        f.write(f"Title: {title}\n\n")
        f.write(cleaned_text)

    print(f"[SAVED] {filepath}")
    return links  # ë§í¬ëŠ” ë°˜í™˜


def extract_links(html, base_url):
    soup = BeautifulSoup(html, "html.parser")
    links = set()
    for tag in soup.find_all("a", href=True):
        href = tag["href"]
        full_url = urljoin(base_url, href)
        parsed = urlparse(full_url)
        if parsed.scheme in ["http", "https"]:
            links.add(full_url)
    return links
